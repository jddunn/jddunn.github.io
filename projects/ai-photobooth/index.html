<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>AI Photobooth</title><meta name="robots" content="index,follow"/><meta name="description" content="undefined"/><meta property="og:title" content="AI Photobooth"/><meta property="og:description" content="undefined"/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="manifest" href="/favicon/site.webmanifest"/><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#000000"/><link rel="shortcut icon" href="/favicon/favicon.ico"/><meta name="msapplication-TileColor" content="#000000"/><meta name="msapplication-config" content="/favicon/browserconfig.xml"/><meta name="theme-color" content="#000"/><link rel="alternate" type="application/rss+xml" href="/feed.xml"/><meta property="og:image" content="https://og-image.vercel.app/Next.js%20Blog%20Starter%20Example.png?theme=light&amp;md=1&amp;fontSize=100px&amp;images=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Ffront%2Fassets%2Fdesign%2Fnextjs-black-logo.svg"/><meta name="next-head-count" content="18"/><link rel="preload" href="https://jddunn.github.io/_next/static/css/102311681b07b408.css" as="style"/><link rel="stylesheet" href="https://jddunn.github.io/_next/static/css/102311681b07b408.css" data-n-g=""/><link rel="preload" href="https://jddunn.github.io/_next/static/css/3116167ddf16f948.css" as="style"/><link rel="stylesheet" href="https://jddunn.github.io/_next/static/css/3116167ddf16f948.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="https://jddunn.github.io/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="https://jddunn.github.io/_next/static/chunks/webpack-ce5f0ec9e7a9015e.js" defer=""></script><script src="https://jddunn.github.io/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="https://jddunn.github.io/_next/static/chunks/main-a028e25a41b5197e.js" defer=""></script><script src="https://jddunn.github.io/_next/static/chunks/pages/_app-f06f56473160e3e4.js" defer=""></script><script src="https://jddunn.github.io/_next/static/chunks/424-a679efd82258b30d.js" defer=""></script><script src="https://jddunn.github.io/_next/static/chunks/283-26f9239c83fd4f05.js" defer=""></script><script src="https://jddunn.github.io/_next/static/chunks/478-04308f6dd89c78fb.js" defer=""></script><script src="https://jddunn.github.io/_next/static/chunks/pages/projects/%5Bslug%5D-2b2fc9a6750ed450.js" defer=""></script><script src="https://jddunn.github.io/_next/static/7nfYsfP6W_HxPI4GB6HRY/_buildManifest.js" defer=""></script><script src="https://jddunn.github.io/_next/static/7nfYsfP6W_HxPI4GB6HRY/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="Navbar_navbar__VkIDk"><nav><section class="mobile-menu md:hidden"><div class="hamburger-icon space-y-2 mt-5 mr-5" style="cursor:pointer;position:fixed;top:0;right:0;margin:20"><span class="block h-0.5 w-8 animate-pulse bg-gray-600"></span><span class="block h-0.5 w-8 animate-pulse bg-gray-600"></span><span class="block h-0.5 w-8 animate-pulse bg-gray-600"></span></div><div class="hideMenuNav"><div class="absolute top-0 right-0 px-8 py-8" style="cursor:pointer"><svg class="h-8 w-8 text-gray-600" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="cursor:pointer"><line x1="18" y1="6" x2="6" y2="18"></line><line x1="6" y1="6" x2="18" y2="18"></line></svg></div><ul class="flex flex-col items-center justify-between min-h-[250px]"><a href="/"><li class="border-b border-gray-400 my-8 uppercase" style="margin-left:0px">HOME</li></a><a href="/about/"><li class="border-b border-gray-400 my-8 uppercase" style="margin-left:0px">ABOUT ME</li></a><a href="/projects/"><li class="border-b border-gray-400 my-8 uppercase" style="margin-left:0px">PROJECTS</li></a><a href="/open-source/"><li class="border-b border-gray-400 my-8 uppercase" style="margin-left:0px">OPEN SOURCE</li></a><a href="/blog/"><li class="border-b border-gray-400 my-8 uppercase" style="margin-left:0px">BLOG</li></a></ul></div></section><ul class="desktop-menu hidden md:flex"><a href="/"><li class="" style="margin-left:0px">__home</li></a><a href="/about/"><li class="" style="margin-left:0px">__about me</li></a><a href="/projects/"><li class="Navbar_active___L3RU" style="margin-left:0px">__projects</li></a><a href="/open-source/"><li class="" style="margin-left:0px">__open source</li></a><a href="/blog/"><li class="" style="margin-left:0px">__blog</li></a></ul></nav><style>
        .hideMenuNav {
          display: none;
        }
        .showMenuNav {
          display: block;
          position: absolute;
          width: 100%;
          min-height: 400px;
          top: 20;
          right: 20;
          z-index: 10;
          display: flex;
          flex-direction: column;
        }
        .mobile-menu ul {
          background-color: rgba(2, 10, 18, 0.95)
        }
    </style></div><div class="min-h-screen"><main><div class="container mx-auto px-5"><h2 class="text-1xl md:text-2xl lg:text-3xl font-bold tracking-tight  md:tracking-tighter leading-tight mb-20 mt-8"></h2><article class="mb-32"><div><h1 class="text-2xl md:text-2xl lg:text-3xl font-bold  tracking-tighter leading-tight md:leading-none mb-12  text-center"><span class="text-2xl md:text-2xl lg:text-3xl red-400 inline tracking-tighter leading-tight md:leading-none mb-12 text-center text-gray-400 mr-2">//</span>AI Photobooth</h1><div class="mb-0"><div class="text-md text-slate-500 text-center">Project created: <time dateTime="2019">2019</time></div><div class="text-md text-slate-200 mb-3 backButton"><button><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M8 0a8 8 0 1 0 0 16A8 8 0 0 0 8 0zm3.5 7.5a.5.5 0 0 1 0 1H5.707l2.147 2.146a.5.5 0 0 1-.708.708l-3-3a.5.5 0 0 1 0-.708l3-3a.5.5 0 1 1 .708.708L5.707 7.5H11.5z"></path></svg></button></div></div><div class="text-md text-slate-500 text-center">Post updated: <time dateTime="2019">2019</time></div><div class="mb-8 md:mb-16 sm:mx-0 mt-5"><div class="sm:mx-0"><div class="relative mx-auto"><img alt="Cover Image for AI Photobooth" loading="lazy" width="600" height="480" decoding="async" data-nimg="1" class="shadow-lg dark:shadow-black/30 mx-auto" style="color:transparent" src="/assets/projects/cat-spirited-away-filter.png"/></div></div><div class="text-sm text-slate-500 mb-4 mt-2 text-center"><span class="mr-5 underline">python</span><span class="mr-5 underline">ai</span><span class="mr-5 underline">computer vision</span><span class="mr-5 underline">photography</span></div></div><div class="max-w-2xl mx-auto"></div></div><div class="max-w-2xl mx-auto"><div class="markdown-styles_markdown__9bgXN"><p><span style="text-align: center; color: grey; margin-left: auto; margin-right: auto; display: block; width: 80%; margin-top: -65px; font-size: .9em">My dear friend, if I can I'll find you again.</span></p>
<p>I was contracted by Edelman to design and create an interactive exhibit for their AI month - one that would utilize <a class="md-link" href="https://en.wikipedia.org/wiki/Neural_style_transfer" target="_blank" style="margin-left: 0; margin-right: 0; display: inline">neural style transfer</a> to provide a fun experience for visitors to their NYC headquarters. I went with the direction of film and pop culture, and thought it would be interesting for people to see themselves transformed into styles from popular media references. I trained 5 different models, 3 of which were chosen for show: Spirited Away, Sin City, Game of Thrones, Blade Runner, and Alice in Wonderland.</p>
<p><a href="/assets/projects/ai-photobooth-demo.gif" target="_blank"><img src="/assets/projects/ai-photobooth-demo.gif" class="img-shadow" style="display: block; margin-left: auto; margin-right: auto;" width="700" alt="Video demo of AI Photobooth UI"></img></a>
<span style="text-align: center; color: grey; margin-left: auto; margin-right: auto; display: block; width: 80%">Video demo of UI.</span></p>
<p>A Python server with a REST API powers the backend and performs the image transformations. After trying the experience, users can enter in their emails and have the photos sent to them via Sendgrid's API. Images can be taken with a web camera or with a mobile / tablet camera (when accessing the website on a mobile device).</p>
<p>I used code samples and research from ywng's <a class="md-link" href="https://github.com/ywng/multi-style-transfer" target="_blank" style="margin-left: 0; margin-right: 0; display: inline">multi-style-transfer</a>, which was a modified implementation of <a class="md-link" href="https://github.com/pytorch/examples/tree/main/fast_neural_style" target="_blank" style="margin-left: 0; margin-right: 0; display: inline">fast-neural-style</a> to allow for multiple images to be used as a reference for style transfer. Although the original purpose of their design was to allow for different types of styles to be transferred, I found that this algorithm also greatly improved results for just a single style transfer (with multiple images of the same style used as reference).</p>
<p>Essentially, I would capture various frames from the sources of media I had, all somewhat cohesive in aesthetic but different in the image content, and trained models those groups of images for a style instead of just a singular one. This made the results of image transformations a lot more consistent across a wider spectrum input images, as it was able to create a "average" sense of details to transform from the data. I had also incorporated <a class="md-link" href="https://github.com/pytorch/examples/tree/main/fast_neural_style" target="_blank" style="margin-left: 0; margin-right: 0; display: inline"> Labeled Faces in the Wild</a> dataset within the base training data, to give greater accuracy for portraits and selfie transformations.</p></div></div></article></div></main></div><div class="Footer_footer__BH5s_"><div><p class="Footer_find__pDm3T" style="letter-spacing:2px">find me on:</p><a href="https://www.linkedin.com/in/jdfive/" target="_blank" rel="noopener noreferrer"><span class="Footer_footerIcon__bo_UL"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854V1.146zm4.943 12.248V6.169H2.542v7.225h2.401zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248-.822 0-1.359.54-1.359 1.248 0 .694.521 1.248 1.327 1.248h.016zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016a5.54 5.54 0 0 1 .016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225h2.4z"></path></svg></span></a><a href="https://github.com/jddunn" target="_blank" rel="noopener noreferrer"><span class="Footer_footerIcon__bo_UL"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path></svg></span></a></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"AI Photobooth","date":"2019","createdDate":"2019","slug":"ai-photobooth","tags":"python,ai,computer vision,photography","content":"\n\u003cspan style=\"text-align: center; color: grey; margin-left: auto; margin-right: auto; display: block; width: 80%; margin-top: -65px; font-size: .9em\"\u003eMy dear friend, if I can I'll find you again.\u003c/span\u003e\n\nI was contracted by Edelman to design and create an interactive exhibit for their AI month - one that would utilize \u003ca class=\"md-link\" href=\"https://en.wikipedia.org/wiki/Neural_style_transfer\" target=\"_blank\" style=\"margin-left: 0; margin-right: 0; display: inline\"\u003eneural style transfer\u003c/a\u003e to provide a fun experience for visitors to their NYC headquarters. I went with the direction of film and pop culture, and thought it would be interesting for people to see themselves transformed into styles from popular media references. I trained 5 different models, 3 of which were chosen for show: Spirited Away, Sin City, Game of Thrones, Blade Runner, and Alice in Wonderland.\n\n\u003ca href=\"/assets/projects/ai-photobooth-demo.gif\" target=\"_blank\"\u003e\u003cimg src=\"/assets/projects/ai-photobooth-demo.gif\" class=\"img-shadow\" style=\"display: block; margin-left: auto; margin-right: auto;\" width=\"700\" alt=\"Video demo of AI Photobooth UI\"\u003e\u003c/img\u003e\u003c/a\u003e\n\u003cspan style=\"text-align: center; color: grey; margin-left: auto; margin-right: auto; display: block; width: 80%\"\u003eVideo demo of UI.\u003c/span\u003e\n\nA Python server with a REST API powers the backend and performs the image transformations. After trying the experience, users can enter in their emails and have the photos sent to them via Sendgrid's API. Images can be taken with a web camera or with a mobile / tablet camera (when accessing the website on a mobile device).\n\nI used code samples and research from ywng's \u003ca class=\"md-link\" href=\"https://github.com/ywng/multi-style-transfer\" target=\"_blank\" style=\"margin-left: 0; margin-right: 0; display: inline\"\u003emulti-style-transfer\u003c/a\u003e, which was a modified implementation of \u003ca class=\"md-link\" href=\"https://github.com/pytorch/examples/tree/main/fast_neural_style\" target=\"_blank\" style=\"margin-left: 0; margin-right: 0; display: inline\"\u003efast-neural-style\u003c/a\u003e to allow for multiple images to be used as a reference for style transfer. Although the original purpose of their design was to allow for different types of styles to be transferred, I found that this algorithm also greatly improved results for just a single style transfer (with multiple images of the same style used as reference).\n\nEssentially, I would capture various frames from the sources of media I had, all somewhat cohesive in aesthetic but different in the image content, and trained models those groups of images for a style instead of just a singular one. This made the results of image transformations a lot more consistent across a wider spectrum input images, as it was able to create a \"average\" sense of details to transform from the data. I had also incorporated \u003ca class=\"md-link\" href=\"https://github.com/pytorch/examples/tree/main/fast_neural_style\" target=\"_blank\" style=\"margin-left: 0; margin-right: 0; display: inline\"\u003e Labeled Faces in the Wild\u003c/a\u003e dataset within the base training data, to give greater accuracy for portraits and selfie transformations.","ogImage":{"url":"/assets/blog/dynamic-routing/cover.jpg"},"coverImage":"/assets/projects/cat-spirited-away-filter.png"}},"__N_SSG":true},"page":"/projects/[slug]","query":{"slug":"ai-photobooth"},"buildId":"7nfYsfP6W_HxPI4GB6HRY","assetPrefix":"https://jddunn.github.io","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>