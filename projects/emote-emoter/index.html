<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>Emote / Emoter</title><meta name="robots" content="index,follow"/><meta name="description" content="undefined"/><meta property="og:title" content="Emote / Emoter"/><meta property="og:description" content="undefined"/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="manifest" href="/favicon/site.webmanifest"/><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#000000"/><link rel="shortcut icon" href="/favicon/favicon.ico"/><meta name="msapplication-TileColor" content="#000000"/><meta name="msapplication-config" content="/favicon/browserconfig.xml"/><meta name="theme-color" content="#000"/><link rel="alternate" type="application/rss+xml" href="/feed.xml"/><meta property="og:image" content="https://og-image.vercel.app/Next.js%20Blog%20Starter%20Example.png?theme=light&amp;md=1&amp;fontSize=100px&amp;images=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Ffront%2Fassets%2Fdesign%2Fnextjs-black-logo.svg"/><meta name="next-head-count" content="18"/><link rel="preload" href="https://jddunn.github.io/_next/static/css/102311681b07b408.css" as="style"/><link rel="stylesheet" href="https://jddunn.github.io/_next/static/css/102311681b07b408.css" data-n-g=""/><link rel="preload" href="https://jddunn.github.io/_next/static/css/3116167ddf16f948.css" as="style"/><link rel="stylesheet" href="https://jddunn.github.io/_next/static/css/3116167ddf16f948.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="https://jddunn.github.io/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="https://jddunn.github.io/_next/static/chunks/webpack-ce5f0ec9e7a9015e.js" defer=""></script><script src="https://jddunn.github.io/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="https://jddunn.github.io/_next/static/chunks/main-a028e25a41b5197e.js" defer=""></script><script src="https://jddunn.github.io/_next/static/chunks/pages/_app-f06f56473160e3e4.js" defer=""></script><script src="https://jddunn.github.io/_next/static/chunks/424-a679efd82258b30d.js" defer=""></script><script src="https://jddunn.github.io/_next/static/chunks/283-26f9239c83fd4f05.js" defer=""></script><script src="https://jddunn.github.io/_next/static/chunks/478-04308f6dd89c78fb.js" defer=""></script><script src="https://jddunn.github.io/_next/static/chunks/pages/projects/%5Bslug%5D-02bcf56bf5fdb1ba.js" defer=""></script><script src="https://jddunn.github.io/_next/static/aLzD5G0tSzCu74-BCNeV7/_buildManifest.js" defer=""></script><script src="https://jddunn.github.io/_next/static/aLzD5G0tSzCu74-BCNeV7/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="Navbar_navbar__VkIDk"><nav><section class="mobile-menu md:hidden"><div class="hamburger-icon space-y-2 mt-5 mr-5" style="cursor:pointer;position:fixed;top:0;right:0;margin:20"><span class="block h-0.5 w-8 animate-pulse bg-gray-600"></span><span class="block h-0.5 w-8 animate-pulse bg-gray-600"></span><span class="block h-0.5 w-8 animate-pulse bg-gray-600"></span></div><div class="hideMenuNav"><div class="absolute top-0 right-0 px-8 py-8" style="cursor:pointer"><svg class="h-8 w-8 text-gray-600" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="cursor:pointer"><line x1="18" y1="6" x2="6" y2="18"></line><line x1="6" y1="6" x2="18" y2="18"></line></svg></div><ul class="flex flex-col items-center justify-between min-h-[250px]"><a href="/"><li class="border-b border-gray-400 my-8 uppercase" style="margin-left:0px">HOME</li></a><a href="/about/"><li class="border-b border-gray-400 my-8 uppercase" style="margin-left:0px">ABOUT ME</li></a><a href="/projects/"><li class="border-b border-gray-400 my-8 uppercase" style="margin-left:0px">PROJECTS</li></a><a href="/open-source/"><li class="border-b border-gray-400 my-8 uppercase" style="margin-left:0px">OPEN SOURCE</li></a><a href="/blog/"><li class="border-b border-gray-400 my-8 uppercase" style="margin-left:0px">BLOG</li></a></ul></div></section><ul class="desktop-menu hidden md:flex"><a href="/"><li class="" style="margin-left:0px">__home</li></a><a href="/about/"><li class="" style="margin-left:0px">__about me</li></a><a href="/projects/"><li class="Navbar_active___L3RU" style="margin-left:0px">__projects</li></a><a href="/open-source/"><li class="" style="margin-left:0px">__open source</li></a><a href="/blog/"><li class="" style="margin-left:0px">__blog</li></a></ul></nav><style>
        .hideMenuNav {
          display: none;
        }
        .showMenuNav {
          display: block;
          position: absolute;
          width: 100%;
          min-height: 400px;
          top: 20;
          right: 20;
          z-index: 10;
          display: flex;
          flex-direction: column;
        }
        .mobile-menu ul {
          background-color: rgba(2, 10, 18, 0.95)
        }
    </style></div><div class="min-h-screen"><main><div class="container mx-auto px-5"><h2 class="text-1xl md:text-2xl lg:text-3xl font-bold tracking-tight  md:tracking-tighter leading-tight mb-20 mt-8"></h2><article class="mb-32"><div><h1 class="text-2xl md:text-2xl lg:text-3xl font-bold  tracking-tighter leading-tight md:leading-none mb-12  text-center"><span class="text-2xl md:text-2xl lg:text-3xl red-400 inline tracking-tighter leading-tight md:leading-none mb-12 text-center text-gray-400 mr-2">//</span>Emote / Emoter</h1><div class="mb-0"><div class="text-md text-slate-500 text-center">Project created: <time dateTime="2017">2017</time></div><div class="text-md text-slate-200 mb-3 backButton"><button><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M8 0a8 8 0 1 0 0 16A8 8 0 0 0 8 0zm3.5 7.5a.5.5 0 0 1 0 1H5.707l2.147 2.146a.5.5 0 0 1-.708.708l-3-3a.5.5 0 0 1 0-.708l3-3a.5.5 0 1 1 .708.708L5.707 7.5H11.5z"></path></svg></button></div></div><div class="text-md text-slate-500 text-center">Post updated: <time dateTime="2017">2017</time></div><div class="mb-8 md:mb-16 sm:mx-0 mt-5"><div class="sm:mx-0"><div class="relative mx-auto"><img alt="Cover Image for Emote / Emoter" loading="lazy" width="600" height="480" decoding="async" data-nimg="1" class="shadow-lg dark:shadow-black/30 mx-auto" style="color:transparent" src="/assets/projects/emote-demo-1.png"/></div></div><div class="text-sm text-slate-500 mb-4 mt-2 text-center"><span class="mr-5 underline">python</span><span class="mr-5 underline">ai</span><span class="mr-5 underline">nlp</span><span class="mr-5 underline">sentiment analysis</span><span class="mr-5 underline">chatbot</span></div></div><div class="max-w-2xl mx-auto"></div></div><div class="max-w-2xl mx-auto"><div class="markdown-styles_markdown__9bgXN"><h2 id="intro">Intro</h2>
<p>Emote / Emoter / A Poor Sort of Memory were all created during my undergraduate thesis at Parsons School of Design. I deployed Emoter in an anachronistic, interactive exhibit (seen below) at the Parson's 2017 BFA thesis show, with a personality that represents myself, created from parsing my archived messages on Facebook.</p>
<p>While the code and dataset was open-source while I was in school, the IP for Emote and Emoter was sold to the startup <a class="md-link" href="https://hereafterlegacy.ai" target="_blank" style="margin-left: 0; margin-right: 0; display: inline">HAL</a>, and so is no longer open-source.</p>
<p><a href="/assets/projects/emoter-talking-demo.gif" target="_blank"><img src="/assets/projects/emoter-talking-demo.gif" class="img-shadow" style="display: block; margin-left: auto; margin-right: auto;" width="800" alt="Emoter digital personality replica chatbot"></img></a></p>
<p>I had originally wanted to create a wearable product integrated with a talking AI, and while looking at all the existing chatbot solutions, I felt dissatisfied. I felt that chatbots should have some way to comprehend messages for some higher meaning, as an actual human mind would. Naturally, empathy seemed like a great way to answer for that higher meaning.</p>
<p>A chatbot with emotional intelligence could not only be able to narrow down schemas in its knowledge database to give more specific, accurate answers, but the bot could also then decipher and understand the emotional context of texts that aren't similar to anything in its training.</p>
<p>I looked to integrate a sentiment analyzer with my bot. I tried a few solutions, and again, felt dissatisfied with them all, including Watson's Tone Analyzer (which, as the name states, is for analyzing the tone of how text comes off, not actually how the person is feeling). Again, I began creating my own, with the help of several open-source libraries.</p>
<h2 id="emotesentimentanalysislibrary">Emote: Sentiment analysis library</h2>
<p>Emote was written in Python and uses the TextBlob / NLTK, NumPy, pandas, and scikit-learn libraries to build a probabilistic sentiment analyzer for 26 different classifications. These classifications have been divided into 13 pairs of opposites, and are designed to be grouped together to create tone clusters that can then encompass more values as well as decrease false positives.</p>
<p>Based off these tone clusters, a further 10 additional tone classifications are derived, allowing for 36 different tones to be detected.</p>
<p><a href="/assets/projects/emoter-empathy-diagram.png" target="_blank"><img src="/assets/projects/emoter-empathy-diagram.png" class="img-shadow" style="display: block; margin-left: auto; margin-right: auto;" width="300" alt="Emote empathy classification chart"></img></a>
<span style="text-align: center; color: grey; margin-left: auto; margin-right: auto; display: block; width: 80%">How the emotional tones were clustered and classified in Emote.</span></p>
<p>I developed the training data for this by classifying ~10,000 quotes of dialog and text from literature and film texts. It was fed into a Naive Bayes classifier. I also developed an algorithm to parse 70,000 of my Facebook messages, in order to "clone" a digital replica of myself that would respond with messages I have said before. It also was integrated with Emote's sentiment analysis, to judge the emotions behind a user's message and respond accordingly with appropriate emotional tones.</p>
<p>A Naive Bayes classifier was chosen for a few reasons, mainly because of the feature-independent probability classification. </p>
<p>Naive Bayes starts with an initial guess based on what it's seen before; it will guess more common classifications more often. This made sense as I had classifications like "positive", "negative", as well as "joyful", or "anger", and the more generalized tones like positive / negative had a greater frequency of labels, and thus were classified more often. </p>
<p>To combat the limitation of potentially only getting classified by the most frequent tones in the training set, I took the highest 3-6 emotional tone probabilities, and normalized them to become more like percentages by reducing the variance between the class values, assuming that if a tone was in the top 5-6 classes found, there was some decent percentage of that tone detected in the text. </p>
<p>To achieve that normalization, I first used the StandardScaler class from scikit-learn's preprocessing methods (which worked effectively despite that it's usually meant for data preprocessing, not normalizing data outputs). I <strong>was</strong> planning to then apply the softmax function to transform those values back into a probability distribution that added up to 1, which exponentiates the probabilities and then divides them by the sum of all exponentiated scores, which would result in even further reduced variance. However, I decided not to, as I didn't want to limit the tones to add up to a 100% percentage, but instead be able to co-exist as a 100% value alongside other tones with percentages, since my detection was meant to analyze multiple tones simultaneously. </p>
<p>Once I had the values from the Standard Scaler transformation, I multiplied them by 100 to achieve a percentage. The Robust Scaler transformation from the same library would be more effective for normalizing outliers, but my class imbalances were not so great that it was needed in my opinion, and by design was supposed to be somewhat imbalanced for more niche tones versus more general ones.</p>
<p>Because I only had ~10k examples in my dataset, the feature independence of the Naive Bayes model worked well, as it simplifies the complexity and thus the amount of data needed to estimate probabilities, while still retaining a high amount of dimensions.</p>
<p>Each n-gram is considered a separate feature in the training, so I had to choose between unigrams, bigrams, trigrams, or further n-grams. Since my dataset was limited, I went with unigrams, despite the fact that no context beyond each word was considered, as the classifier would still take into account the frequency of every word (or n-gram) in the text. For me, the tradeoff for higher accuracy in a smaller dataset was worth the classifier risking inaccuracies when analyzing phrases like "that was so not good", versus "not! that was so good", which I figured would often be fringe cases.</p>
<p>In the future, a Naive Bayes model based on bigrams or even trigrams with a 100k examples in the training dataset would be more versatile and presumably more accurate than this model.</p>
<h2 id="emoteimagegallery">Emote image gallery</h2>
<p><a href="/assets/projects/emote-demo-2.png" target="_blank"><img src="/assets/projects/emote-demo-2.png" class="img-shadow" style="display: block; margin-left: auto; margin-right: auto;" width="500" alt="Emote web interface demo"></img></a>
<span style="text-align: center; color: grey; margin-left: auto; margin-right: auto; display: block; width: 80%">Emote has a web interface built in Flask and with Bootstrap.</span></p>
<p><a href="/assets/projects/emote-demo-4.png" target="_blank"><img src="/assets/projects/emote-demo-4.png" class="img-shadow" style="display: block; margin-left: auto; margin-right: auto;" width="700" alt="Emote bulk analysis feature"></img></a>
<span style="text-align: center; color: grey; margin-left: auto; margin-right: auto; display: block; width: 80%">Emote has a mass analysis feature for analyzing CSV or text files, including PDFs / books.</span></p>
<h2 id="emoterchatbotswithempathyandmemory">Emoter: Chatbots with empathy and memory</h2>
<p>Emoter is a basic but functional chatbot platform intergrated with Emote (also in Python), in order to give chatbot agents the ability to empathize with users and give back emotionally appropriate responses.</p>
<p>Emoter agents thus can operate on a "higher level of thinking", by first categorizing messages and then choosing specific, interchangeable "conversations" (lists of text responses) to respond from based on certain emotional tones.</p>
<p>Within these conversations, Emoter looks for matching text in its database and compares it with the user input on a sliding threshold, outputting the corresponding response if the threshold is met. This "message-response" pair matching algorithm was simple (by design, as the NLP for matching responses in a conversation was not the focal point of hte project): It used cosine similarity after extracting the most important text words after some tf-idf (term frequency - inverse document frequency) analysis, which measures the importance of a word by weighing how often it appears in a single document then offsetting that by the frequency of the word's appearance in the entire corpus.</p>
<p>The combined approach of cosine similarity and tf-idf filtering worked well, as the similarity threshold required for matching an appropriate response was much lower than if it checked for the entire text without filtering, so often multiple appropriate responses were matched even with a relatively small dataset, and the response chosen was randomized from the appropriate ones found, allowing for a dynamic conversation experience.</p>
<p><a href="/assets/projects/emoter_demo_5.png" target="_blank"><img src="/assets/projects/emoter_demo_5.png" class="img-shadow" style="display: block; margin-left: auto; margin-right: auto;" width="800" alt="Emoter agent with the personality of a fitness coach"></img></a>
<span style="text-align: center; color: grey; margin-left: auto; margin-right: auto; display: block; width: 80%">A demo 'Emoter agent' with a persona of a fitness coach.</span></p>
<p>In Emoter's training data, values can be passed into the list of lists (2D array) after any input-response pair, and checked for before Emoter gives out a response. In my interactive fiction game demo Eden, I used this to create a branching narrative driven by the mechanic of talking to the only other interactable character.</p>
<p>The Emoter bot was able to keep track of whether or not certain things were said by the user previously, and how often each thing was said. Thus, Emoter bots can be written with limited short-term "memory" features, so that they can continue speaking on the same conversations.</p>
<p><a href="/assets/projects/eden_demo_3.png" target="_blank"><img src="/assets/projects/eden_demo_3.png" class="img-shadow" style="display: block; margin-left: auto; margin-right: auto;" width="600" alt="Emoter agent in interactive fiction game Eden"></img></a></p>
<h2 id="apoorsortofmemoryorhowtobuildadigitalreplicaofyourselfwithempathy">A poor sort of memory, or, how to build a digital replica of yourself with empathy</h2>
<p>Finally, while developing Emoter, I became more fascinated with the idea of automatically generating digital personalities of individual people, specifically given their archived data on social media (inspired by Black Mirror's episode Be Right Back. I developed an algorithm to parse 70,000 of my Facebook messages (downloaded from their official service), to create a database Emoter could use that was specifically mined from my words. This project was deployed as an interactive exhibit in a gallery.</p>
<p><a href="/assets/projects/emote-facebook-messages.png" target="_blank"><img src="/assets/projects/emote-facebook-messages.png" class="img-shadow" style="display: block; margin-left: auto; margin-right: auto;" width="800" alt="Emoter Facebook messages cloning></img></a>
<span style="text-align: center; color: grey; margin-left: auto; margin-right: auto; display: block; width: 80%">Facebook message records transformed into message-pair responses for the chatbot using a parsing algorithm.</span></p>
<p><a href="/assets/projects/emoter-web-demo-1.png" target="_blank"><img src="/assets/projects/emoter-web-demo-1.png" class="img-shadow" style="display: block; margin-left: auto; margin-right: auto;" width="800" alt="Emoter personality clone demo 1"></img></a></p>
<p><a href="/assets/projects/emoter-web-demo-2.png" target="_blank"><img src="/assets/projects/emoter-web-demo-2.png" class="img-shadow" style="display: block; margin-left: auto; margin-right: auto;" width="800" alt="Emoter personality clone demo 2"></img></a></p>
<p><a href="/assets/projects/a_poor_sort_of_memory_only_backwards.jpeg" target="_blank"><img src="/assets/projects/a_poor_sort_of_memory_only_backwards.jpeg" class="img-shadow" style="display: block; margin-left: auto; margin-right: auto;" width="800" alt="Emoter personality clone demo 2"></img></a>
<span style="text-align: center; color: grey; margin-left: auto; margin-right: auto; display: block; width: 80%">A poor sort of memory that only works backwards.</span></p>
<p><b>Users talking to the chatbot during the gallery, and the chatbot demonstrating emotional understanding of the message intents and responding accordingly (logs saved from the backend):</b></p>
<p><a href="/assets/projects/emoter-demo-1.png" target="_blank"><img src="/assets/projects/emoter-demo-1.png" class="img-shadow" style="display: block; margin-left: auto; margin-right: auto;" width="800" alt="Emoter personality clone empathy demo 1"></img></a></p>
<p><a href="/assets/projects/emoter-demo-2.png" target="_blank"><img src="/assets/projects/emoter-demo-2.png" class="img-shadow" style="display: block; margin-left: auto; margin-right: auto;" width="800" alt="Emoter personality clone empathy demo 2"></img></a></p>
<p><a href="/assets/projects/emoter-demo-3.png" target="_blank"><img src="/assets/projects/emoter-demo-3.png" class="img-shadow" style="display: block; margin-left: auto; margin-right: auto;" width="800" alt="Emoter personality clone empathy demo 3"></img></a></p></div></div></article></div></main></div><div class="Footer_footer__BH5s_"><div><p class="Footer_find__pDm3T" style="letter-spacing:2px">find me on:</p><a href="https://www.linkedin.com/in/jdfive/" target="_blank" rel="noopener noreferrer"><span class="Footer_footerIcon__bo_UL"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854V1.146zm4.943 12.248V6.169H2.542v7.225h2.401zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248-.822 0-1.359.54-1.359 1.248 0 .694.521 1.248 1.327 1.248h.016zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016a5.54 5.54 0 0 1 .016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225h2.4z"></path></svg></span></a><a href="https://github.com/jddunn" target="_blank" rel="noopener noreferrer"><span class="Footer_footerIcon__bo_UL"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path></svg></span></a></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"Emote / Emoter","date":"2017","createdDate":"2017","slug":"emote-emoter","tags":"python,ai,nlp,sentiment analysis,chatbot","content":"## Intro\n\nEmote / Emoter / A Poor Sort of Memory were all created during my undergraduate thesis at Parsons School of Design. I deployed Emoter in an anachronistic, interactive exhibit (seen below) at the Parson's 2017 BFA thesis show, with a personality that represents myself, created from parsing my archived messages on Facebook.\n\nWhile the code and dataset was open-source while I was in school, the IP for Emote and Emoter was sold to the startup \u003ca class=\"md-link\" href=\"https://hereafterlegacy.ai\" target=\"_blank\" style=\"margin-left: 0; margin-right: 0; display: inline\"\u003eHAL\u003c/a\u003e, and so is no longer open-source.\n\n\u003ca href=\"/assets/projects/emoter-talking-demo.gif\" target=\"_blank\"\u003e\u003cimg src=\"/assets/projects/emoter-talking-demo.gif\" class=\"img-shadow\" style=\"display: block; margin-left: auto; margin-right: auto;\" width=\"800\" alt=\"Emoter digital personality replica chatbot\"\u003e\u003c/img\u003e\u003c/a\u003e\n\nI had originally wanted to create a wearable product integrated with a talking AI, and while looking at all the existing chatbot solutions, I felt dissatisfied. I felt that chatbots should have some way to comprehend messages for some higher meaning, as an actual human mind would. Naturally, empathy seemed like a great way to answer for that higher meaning.\n\nA chatbot with emotional intelligence could not only be able to narrow down schemas in its knowledge database to give more specific, accurate answers, but the bot could also then decipher and understand the emotional context of texts that aren't similar to anything in its training.\n\nI looked to integrate a sentiment analyzer with my bot. I tried a few solutions, and again, felt dissatisfied with them all, including Watson's Tone Analyzer (which, as the name states, is for analyzing the tone of how text comes off, not actually how the person is feeling). Again, I began creating my own, with the help of several open-source libraries.\n\n## Emote: Sentiment analysis library\n\nEmote was written in Python and uses the TextBlob / NLTK, NumPy, pandas, and scikit-learn libraries to build a probabilistic sentiment analyzer for 26 different classifications. These classifications have been divided into 13 pairs of opposites, and are designed to be grouped together to create tone clusters that can then encompass more values as well as decrease false positives.\n\nBased off these tone clusters, a further 10 additional tone classifications are derived, allowing for 36 different tones to be detected.\n\n\u003ca href=\"/assets/projects/emoter-empathy-diagram.png\" target=\"_blank\"\u003e\u003cimg src=\"/assets/projects/emoter-empathy-diagram.png\" class=\"img-shadow\" style=\"display: block; margin-left: auto; margin-right: auto;\" width=\"300\" alt=\"Emote empathy classification chart\"\u003e\u003c/img\u003e\u003c/a\u003e\n\u003cspan style=\"text-align: center; color: grey; margin-left: auto; margin-right: auto; display: block; width: 80%\"\u003eHow the emotional tones were clustered and classified in Emote.\u003c/span\u003e\n\nI developed the training data for this by classifying ~10,000 quotes of dialog and text from literature and film texts. It was fed into a Naive Bayes classifier. I also developed an algorithm to parse 70,000 of my Facebook messages, in order to \"clone\" a digital replica of myself that would respond with messages I have said before. It also was integrated with Emote's sentiment analysis, to judge the emotions behind a user's message and respond accordingly with appropriate emotional tones.\n\nA Naive Bayes classifier was chosen for a few reasons, mainly because of the feature-independent probability classification. \n\nNaive Bayes starts with an initial guess based on what it's seen before; it will guess more common classifications more often. This made sense as I had classifications like \"positive\", \"negative\", as well as \"joyful\", or \"anger\", and the more generalized tones like positive / negative had a greater frequency of labels, and thus were classified more often. \n\nTo combat the limitation of potentially only getting classified by the most frequent tones in the training set, I took the highest 3-6 emotional tone probabilities, and normalized them to become more like percentages by reducing the variance between the class values, assuming that if a tone was in the top 5-6 classes found, there was some decent percentage of that tone detected in the text. \n\nTo achieve that normalization, I first used the StandardScaler class from scikit-learn's preprocessing methods (which worked effectively despite that it's usually meant for data preprocessing, not normalizing data outputs). I **was** planning to then apply the softmax function to transform those values back into a probability distribution that added up to 1, which exponentiates the probabilities and then divides them by the sum of all exponentiated scores, which would result in even further reduced variance. However, I decided not to, as I didn't want to limit the tones to add up to a 100% percentage, but instead be able to co-exist as a 100% value alongside other tones with percentages, since my detection was meant to analyze multiple tones simultaneously. \n\nOnce I had the values from the Standard Scaler transformation, I multiplied them by 100 to achieve a percentage. The Robust Scaler transformation from the same library would be more effective for normalizing outliers, but my class imbalances were not so great that it was needed in my opinion, and by design was supposed to be somewhat imbalanced for more niche tones versus more general ones.\n\nBecause I only had ~10k examples in my dataset, the feature independence of the Naive Bayes model worked well, as it simplifies the complexity and thus the amount of data needed to estimate probabilities, while still retaining a high amount of dimensions.\n\nEach n-gram is considered a separate feature in the training, so I had to choose between unigrams, bigrams, trigrams, or further n-grams. Since my dataset was limited, I went with unigrams, despite the fact that no context beyond each word was considered, as the classifier would still take into account the frequency of every word (or n-gram) in the text. For me, the tradeoff for higher accuracy in a smaller dataset was worth the classifier risking inaccuracies when analyzing phrases like \"that was so not good\", versus \"not! that was so good\", which I figured would often be fringe cases.\n\nIn the future, a Naive Bayes model based on bigrams or even trigrams with a 100k examples in the training dataset would be more versatile and presumably more accurate than this model.\n\n## Emote image gallery\n\n\u003ca href=\"/assets/projects/emote-demo-2.png\" target=\"_blank\"\u003e\u003cimg src=\"/assets/projects/emote-demo-2.png\" class=\"img-shadow\" style=\"display: block; margin-left: auto; margin-right: auto;\" width=\"500\" alt=\"Emote web interface demo\"\u003e\u003c/img\u003e\u003c/a\u003e\n\u003cspan style=\"text-align: center; color: grey; margin-left: auto; margin-right: auto; display: block; width: 80%\"\u003eEmote has a web interface built in Flask and with Bootstrap.\u003c/span\u003e\n\n\u003ca href=\"/assets/projects/emote-demo-4.png\" target=\"_blank\"\u003e\u003cimg src=\"/assets/projects/emote-demo-4.png\" class=\"img-shadow\" style=\"display: block; margin-left: auto; margin-right: auto;\" width=\"700\" alt=\"Emote bulk analysis feature\"\u003e\u003c/img\u003e\u003c/a\u003e\n\u003cspan style=\"text-align: center; color: grey; margin-left: auto; margin-right: auto; display: block; width: 80%\"\u003eEmote has a mass analysis feature for analyzing CSV or text files, including PDFs / books.\u003c/span\u003e\n\n## Emoter: Chatbots with empathy and memory\n\nEmoter is a basic but functional chatbot platform intergrated with Emote (also in Python), in order to give chatbot agents the ability to empathize with users and give back emotionally appropriate responses.\n\nEmoter agents thus can operate on a \"higher level of thinking\", by first categorizing messages and then choosing specific, interchangeable \"conversations\" (lists of text responses) to respond from based on certain emotional tones.\n\nWithin these conversations, Emoter looks for matching text in its database and compares it with the user input on a sliding threshold, outputting the corresponding response if the threshold is met. This \"message-response\" pair matching algorithm was simple (by design, as the NLP for matching responses in a conversation was not the focal point of hte project): It used cosine similarity after extracting the most important text words after some tf-idf (term frequency - inverse document frequency) analysis, which measures the importance of a word by weighing how often it appears in a single document then offsetting that by the frequency of the word's appearance in the entire corpus.\n\nThe combined approach of cosine similarity and tf-idf filtering worked well, as the similarity threshold required for matching an appropriate response was much lower than if it checked for the entire text without filtering, so often multiple appropriate responses were matched even with a relatively small dataset, and the response chosen was randomized from the appropriate ones found, allowing for a dynamic conversation experience.\n\n\u003ca href=\"/assets/projects/emoter_demo_5.png\" target=\"_blank\"\u003e\u003cimg src=\"/assets/projects/emoter_demo_5.png\" class=\"img-shadow\" style=\"display: block; margin-left: auto; margin-right: auto;\" width=\"800\" alt=\"Emoter agent with the personality of a fitness coach\"\u003e\u003c/img\u003e\u003c/a\u003e\n\u003cspan style=\"text-align: center; color: grey; margin-left: auto; margin-right: auto; display: block; width: 80%\"\u003eA demo 'Emoter agent' with a persona of a fitness coach.\u003c/span\u003e\n\nIn Emoter's training data, values can be passed into the list of lists (2D array) after any input-response pair, and checked for before Emoter gives out a response. In my interactive fiction game demo Eden, I used this to create a branching narrative driven by the mechanic of talking to the only other interactable character.\n\nThe Emoter bot was able to keep track of whether or not certain things were said by the user previously, and how often each thing was said. Thus, Emoter bots can be written with limited short-term \"memory\" features, so that they can continue speaking on the same conversations.\n\n\u003ca href=\"/assets/projects/eden_demo_3.png\" target=\"_blank\"\u003e\u003cimg src=\"/assets/projects/eden_demo_3.png\" class=\"img-shadow\" style=\"display: block; margin-left: auto; margin-right: auto;\" width=\"600\" alt=\"Emoter agent in interactive fiction game Eden\"\u003e\u003c/img\u003e\u003c/a\u003e\n\n## A poor sort of memory, or, how to build a digital replica of yourself with empathy\n\nFinally, while developing Emoter, I became more fascinated with the idea of automatically generating digital personalities of individual people, specifically given their archived data on social media (inspired by Black Mirror's episode Be Right Back. I developed an algorithm to parse 70,000 of my Facebook messages (downloaded from their official service), to create a database Emoter could use that was specifically mined from my words. This project was deployed as an interactive exhibit in a gallery.\n\n\u003ca href=\"/assets/projects/emote-facebook-messages.png\" target=\"_blank\"\u003e\u003cimg src=\"/assets/projects/emote-facebook-messages.png\" class=\"img-shadow\" style=\"display: block; margin-left: auto; margin-right: auto;\" width=\"800\" alt=\"Emoter Facebook messages cloning\u003e\u003c/img\u003e\u003c/a\u003e\n\u003cspan style=\"text-align: center; color: grey; margin-left: auto; margin-right: auto; display: block; width: 80%\"\u003eFacebook message records transformed into message-pair responses for the chatbot using a parsing algorithm.\u003c/span\u003e\n\n\u003ca href=\"/assets/projects/emoter-web-demo-1.png\" target=\"_blank\"\u003e\u003cimg src=\"/assets/projects/emoter-web-demo-1.png\" class=\"img-shadow\" style=\"display: block; margin-left: auto; margin-right: auto;\" width=\"800\" alt=\"Emoter personality clone demo 1\"\u003e\u003c/img\u003e\u003c/a\u003e\n\n\u003ca href=\"/assets/projects/emoter-web-demo-2.png\" target=\"_blank\"\u003e\u003cimg src=\"/assets/projects/emoter-web-demo-2.png\" class=\"img-shadow\" style=\"display: block; margin-left: auto; margin-right: auto;\" width=\"800\" alt=\"Emoter personality clone demo 2\"\u003e\u003c/img\u003e\u003c/a\u003e\n\n\u003ca href=\"/assets/projects/a_poor_sort_of_memory_only_backwards.jpeg\" target=\"_blank\"\u003e\u003cimg src=\"/assets/projects/a_poor_sort_of_memory_only_backwards.jpeg\" class=\"img-shadow\" style=\"display: block; margin-left: auto; margin-right: auto;\" width=\"800\" alt=\"Emoter personality clone demo 2\"\u003e\u003c/img\u003e\u003c/a\u003e\n\u003cspan style=\"text-align: center; color: grey; margin-left: auto; margin-right: auto; display: block; width: 80%\"\u003eA poor sort of memory that only works backwards.\u003c/span\u003e\n\n\u003cb\u003eUsers talking to the chatbot during the gallery, and the chatbot demonstrating emotional understanding of the message intents and responding accordingly (logs saved from the backend):\u003c/b\u003e\n\n\u003ca href=\"/assets/projects/emoter-demo-1.png\" target=\"_blank\"\u003e\u003cimg src=\"/assets/projects/emoter-demo-1.png\" class=\"img-shadow\" style=\"display: block; margin-left: auto; margin-right: auto;\" width=\"800\" alt=\"Emoter personality clone empathy demo 1\"\u003e\u003c/img\u003e\u003c/a\u003e\n\n\n\u003ca href=\"/assets/projects/emoter-demo-2.png\" target=\"_blank\"\u003e\u003cimg src=\"/assets/projects/emoter-demo-2.png\" class=\"img-shadow\" style=\"display: block; margin-left: auto; margin-right: auto;\" width=\"800\" alt=\"Emoter personality clone empathy demo 2\"\u003e\u003c/img\u003e\u003c/a\u003e\n\n\u003ca href=\"/assets/projects/emoter-demo-3.png\" target=\"_blank\"\u003e\u003cimg src=\"/assets/projects/emoter-demo-3.png\" class=\"img-shadow\" style=\"display: block; margin-left: auto; margin-right: auto;\" width=\"800\" alt=\"Emoter personality clone empathy demo 3\"\u003e\u003c/img\u003e\u003c/a\u003e\n","ogImage":{"url":"/assets/projects/emote-demo-1.png"},"coverImage":"/assets/projects/emote-demo-1.png"}},"__N_SSG":true},"page":"/projects/[slug]","query":{"slug":"emote-emoter"},"buildId":"aLzD5G0tSzCu74-BCNeV7","assetPrefix":"https://jddunn.github.io","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>